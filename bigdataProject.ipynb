{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -q9 > /dev/null\n",
        "#Install latest Version of pyspark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "#Set environment variable for colab\n",
        "import os\n",
        "os. environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "S5dxTivBLyV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUTSte8cMJ1i",
        "outputId": "43b1f928-d2de-4b94-f109-6c017bda1984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
      ],
      "metadata": {
        "id": "uUxIGu4pMYHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're trying this out with EMR cluster, use the below provided code and paste it in the terminal's python file, One change you need to do is in csv_file_path variable below, use this URI of the s3 bucket where you've uploaded the dataset and make sure you've given necessary permissions to S3 bucket so that we can access them into our cluster. After these changes, run this with spark-submit file_name.py command and you can see the output.  "
      ],
      "metadata": {
        "id": "0ismhAUpPFxx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNnV8tT7Gvet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4348d0c3-8d8c-449f-ab62-2a892d3dff77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error on Test Set: 3126.045940173496\n",
            "+---------------+-------------------+\n",
            "|COVID-19 Deaths|         prediction|\n",
            "+---------------+-------------------+\n",
            "|              0|0.04616758426014178|\n",
            "|            120| 119.67256404225512|\n",
            "|              0|0.04616758426014178|\n",
            "|            120| 119.67256404225512|\n",
            "|            120| 119.67256404225512|\n",
            "|            120| 119.67256404225512|\n",
            "|              0|0.04616758426014178|\n",
            "|              0|0.04616758426014178|\n",
            "|              0|0.04616758426014178|\n",
            "|              0|0.04616758426014178|\n",
            "|              0|0.04616758426014178|\n",
            "|            120| 119.67256404225512|\n",
            "|              0|0.04616758426014178|\n",
            "|            120| 119.67256404225512|\n",
            "|              0|0.04616758426014178|\n",
            "|            120| 119.67256404225512|\n",
            "|              0|0.04616758426014178|\n",
            "|            120| 119.67256404225512|\n",
            "|            120| 119.67256404225512|\n",
            "|            120| 119.67256404225512|\n",
            "+---------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.sql.functions import col,isnan, when, count\n",
        "from pyspark.ml.feature import StringIndexer, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler, IndexToString\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "csv_file_path = \"/mys3bucket/location.csv\"\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CovidAnalysis\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "\n",
        "columns_to_drop = ['Data As Of', 'Start Date', 'End Date', 'ICD10_codes']\n",
        "df = df.drop(*columns_to_drop)\n",
        "\n",
        "numeric_cols = ['Year', 'Month', 'COVID-19 Deaths', 'Number of Mentions']\n",
        "categorical_cols = ['Group', 'State', 'Condition Group', 'Condition', 'Age Group']\n",
        "\n",
        "# Fill missing values in numeric columns with mean\n",
        "for col_name in numeric_cols:\n",
        "    mean_value = df.agg({col_name: 'mean'}).collect()[0][0]\n",
        "    df = df.na.fill(mean_value, [col_name])\n",
        "\n",
        "\n",
        "# Fill missing values in categorical columns with mode\n",
        "for col_name in categorical_cols:\n",
        "    mode_value = df.groupBy(col_name).count().orderBy(col(\"count\").desc()).first()[0]\n",
        "    df = df.na.fill(mode_value, [col_name])\n",
        "\n",
        "categorical_cols = ['Group', 'State', 'Condition Group', 'Condition', 'Age Group']\n",
        "\n",
        "# Indexing categorical columns\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"skip\") for col in categorical_cols]\n",
        "\n",
        "# Creating a Pipeline for indexing categorical columns\n",
        "indexer_pipeline = Pipeline(stages=indexers)\n",
        "df_indexed = indexer_pipeline.fit(df).transform(df)\n",
        "\n",
        "# Assembling indexed categorical columns into a single vector\n",
        "cat_cols_indexed = [col+\"_index\" for col in categorical_cols]\n",
        "assembler = VectorAssembler(inputCols=cat_cols_indexed, outputCol=\"cat_features_raw\")\n",
        "df_assembled = assembler.transform(df_indexed)\n",
        "\n",
        "# Scaling categorical features\n",
        "scaler = StandardScaler(inputCol=\"cat_features_raw\", outputCol=\"cat_features\", withMean=True, withStd=True)\n",
        "scaler_model = scaler.fit(df_assembled)\n",
        "df = scaler_model.transform(df_assembled)\n",
        "\n",
        "# Selecting only relevant columns\n",
        "selected_cols = cat_cols_indexed + [\"cat_features\"]\n",
        "df_result = df.select(*selected_cols)\n",
        "\n",
        "# Scale numerical features\n",
        "numerical_cols = ['Year', 'Month',  'Number of Mentions']\n",
        "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"num_features_raw\")\n",
        "scaler = StandardScaler(inputCol=\"num_features_raw\", outputCol=\"num_features\", withStd=True, withMean=True)\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, scaler])\n",
        "df = pipeline.fit(df).transform(df)\n",
        "\n",
        "cat_features = ['cat_features']\n",
        "num_features = ['num_features']\n",
        "\n",
        "# Assembling categorical and numeric columns into a single feature vector column\n",
        "assembler = VectorAssembler(inputCols=cat_features + num_features, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Selecting the relevant columns for the model\n",
        "selected_cols = ['features', 'COVID-19 Deaths']\n",
        "df_selected = df.select(*selected_cols)\n",
        "\n",
        "train_data, test_data = df_selected.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "gbt = GBTRegressor(labelCol='COVID-19 Deaths', featuresCol='features', maxDepth=5, maxIter=20, seed=42)\n",
        "\n",
        "pipeline = Pipeline(stages=[gbt])\n",
        "\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "Train_predictions = model.transform(train_data)\n",
        "\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = RegressionEvaluator(labelCol='COVID-19 Deaths', predictionCol='prediction', metricName='rmse')\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f'Root Mean Squared Error on Test Set: {rmse}')\n",
        "\n",
        "predictions.select(\"COVID-19 Deaths\", \"prediction\").show()"
      ]
    }
  ]
}